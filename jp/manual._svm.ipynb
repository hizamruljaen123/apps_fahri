{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:\n",
      "(508, 6)\n",
      "(508,)\n",
      "Testing set shape:\n",
      "(127, 6)\n",
      "(127,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load data from Excel file\n",
    "data_path = '../data_latih.xlsx'\n",
    "data_latih = pd.read_excel(data_path)\n",
    "\n",
    "# Encoding data kategorikal\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode 'L/P'\n",
    "data_latih['L/P'] = le.fit_transform(data_latih['L/P'])\n",
    "\n",
    "# Encode 'Penghasilan'\n",
    "penghasilan_mapping = {\n",
    "    'Rp. 500,000 - Rp. 999,999': 0,\n",
    "    'Rp. 1,000,000 - Rp. 1,999,999': 1,\n",
    "    'Rp. 2,000,000 - Rp. 4,999,999': 2,\n",
    "    'Tidak Berpenghasilan': 3,\n",
    "    'Kurang dari Rp. 500,000': 4\n",
    "}\n",
    "data_latih['Penghasilan'] = data_latih['Penghasilan'].map(penghasilan_mapping)\n",
    "\n",
    "# Encode 'Status Ekonomi'\n",
    "status_ekonomi_mapping = {\n",
    "    'SANGAT MISKIN': 0,\n",
    "    'MISKIN': 1,\n",
    "    'CUKUP': 2\n",
    "}\n",
    "data_latih['Status Ekonomi'] = data_latih['Status Ekonomi'].map(status_ekonomi_mapping)\n",
    "\n",
    "# Encode 'Layak PIP'\n",
    "layak_pip_mapping = {'Ya': 1, 'Tidak': 0}\n",
    "data_latih['Layak PIP'] = data_latih['Layak PIP'].map(layak_pip_mapping)\n",
    "\n",
    "# Drop columns that are not used in the model\n",
    "data_latih = data_latih.drop(columns=['Nama', 'Alasan Layak PIP', 'Status Bantuan', 'Status Kesesuaian'])\n",
    "\n",
    "# Pisahkan fitur dan target\n",
    "X = data_latih.drop(columns=['Layak PIP'])\n",
    "y = data_latih['Layak PIP']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Menampilkan shapes dari training dan testing set\n",
    "print(\"Training set shape:\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Testing set shape:\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of X_train_scaled:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L/P</th>\n",
       "      <th>Penghasilan</th>\n",
       "      <th>Status Ekonomi</th>\n",
       "      <th>Jumlah Tanggungan</th>\n",
       "      <th>Tahun Penerimaan</th>\n",
       "      <th>Jumlah Bantuan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.691953</td>\n",
       "      <td>1.225137</td>\n",
       "      <td>-0.301944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.785716</td>\n",
       "      <td>-1.264341</td>\n",
       "      <td>-0.905832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.879478</td>\n",
       "      <td>1.225137</td>\n",
       "      <td>1.509720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026759</td>\n",
       "      <td>1.225137</td>\n",
       "      <td>-0.301944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.785716</td>\n",
       "      <td>1.225137</td>\n",
       "      <td>-0.905832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L/P  Penghasilan  Status Ekonomi  Jumlah Tanggungan  Tahun Penerimaan  \\\n",
       "0  0.0     2.691953        1.225137          -0.301944               0.0   \n",
       "1  0.0     1.785716       -1.264341          -0.905832               0.0   \n",
       "2  0.0     0.879478        1.225137           1.509720               0.0   \n",
       "3  0.0    -0.026759        1.225137          -0.301944               0.0   \n",
       "4  0.0     1.785716        1.225137          -0.905832               0.0   \n",
       "\n",
       "   Jumlah Bantuan  \n",
       "0             1.0  \n",
       "1            -1.0  \n",
       "2             1.0  \n",
       "3             1.0  \n",
       "4            -1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of X_test_scaled:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L/P</th>\n",
       "      <th>Penghasilan</th>\n",
       "      <th>Status Ekonomi</th>\n",
       "      <th>Jumlah Tanggungan</th>\n",
       "      <th>Tahun Penerimaan</th>\n",
       "      <th>Jumlah Bantuan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026759</td>\n",
       "      <td>-1.264341</td>\n",
       "      <td>1.509720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026759</td>\n",
       "      <td>1.225137</td>\n",
       "      <td>-0.905832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.932996</td>\n",
       "      <td>-0.019602</td>\n",
       "      <td>0.905832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.932996</td>\n",
       "      <td>-1.264341</td>\n",
       "      <td>0.905832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.932996</td>\n",
       "      <td>-1.264341</td>\n",
       "      <td>-0.301944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L/P  Penghasilan  Status Ekonomi  Jumlah Tanggungan  Tahun Penerimaan  \\\n",
       "0  0.0    -0.026759       -1.264341           1.509720               0.0   \n",
       "1  0.0    -0.026759        1.225137          -0.905832               0.0   \n",
       "2  0.0    -0.932996       -0.019602           0.905832               0.0   \n",
       "3  0.0    -0.932996       -1.264341           0.905832               0.0   \n",
       "4  0.0    -0.932996       -1.264341          -0.301944               0.0   \n",
       "\n",
       "   Jumlah Bantuan  \n",
       "0             1.0  \n",
       "1             1.0  \n",
       "2             1.0  \n",
       "3            -1.0  \n",
       "4            -1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalisasi kolom numerik\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "print(\"First few rows of X_train_scaled:\")\n",
    "display(X_train_scaled.head())\n",
    "print(\"First few rows of X_test_scaled:\")\n",
    "display(X_test_scaled.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:\n",
      "(241, 6)\n",
      "(241,)\n",
      "Testing set shape:\n",
      "(61, 6)\n",
      "(61,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pisahkan fitur dan target\n",
    "X = data_latih.drop(columns=['Nama', 'Layak PIP', 'Alasan Layak PIP', 'Status Bantuan', 'Status Kesesuaian'])\n",
    "y = data_latih['Layak PIP'].map({'Ya': 1, 'Tidak': 0})\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Menampilkan shapes dari training dan testing set\n",
    "print(\"Training set shape:\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Testing set shape:\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights (w): [0. 0. 0. 0. 0. 0.]\n",
      "Initial bias (b): 0\n",
      "Learning rate: 0.01\n",
      "Number of iterations: 1000\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi parameter\n",
    "\n",
    "# Bobot awal diinisialisasi dengan nilai nol untuk semua fitur\n",
    "w = np.zeros(X_train_scaled.shape[1])\n",
    "\n",
    "# Bias awal diinisialisasi dengan nilai nol\n",
    "b = 0\n",
    "\n",
    "# Learning rate (alpha) menentukan seberapa besar langkah yang diambil pada setiap iterasi saat memperbarui bobot dan bias\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Jumlah iterasi menentukan berapa kali proses pembaruan bobot dan bias dilakukan\n",
    "n_iterations = 1000\n",
    "\n",
    "# Menampilkan parameter awal\n",
    "print(\"Initial weights (w):\", w)\n",
    "print(\"Initial bias (b):\", b)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Number of iterations:\", n_iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hinge loss: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk menghitung hinge loss\n",
    "def hinge_loss(X, y, w, b):\n",
    "    # Hinge loss dihitung dengan maks(0, 1 - y * (X * w + b))\n",
    "    return np.maximum(0, 1 - y * (np.dot(X, w) + b))\n",
    "\n",
    "# Menghitung hinge loss untuk parameter awal\n",
    "X_train_array = X_train_scaled.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "\n",
    "initial_loss = hinge_loss(X_train_array, y_train_array, w, b)\n",
    "print(\"Initial hinge loss:\", initial_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100:\n",
      "Weights: [5.1991989 5.1991989 5.1991989 5.1991989 5.1991989 5.1991989]\n",
      "Bias: -1.7000000000000013\n",
      "\n",
      "Iteration 2/100:\n",
      "Weights: [9.36888001 9.36888001 9.36888001 9.36888001 9.36888001 9.36888001]\n",
      "Bias: -3.1999999999999758\n",
      "\n",
      "Iteration 3/100:\n",
      "Weights: [13.04338916 13.04338916 13.04338916 13.04338916 13.04338916 13.04338916]\n",
      "Bias: -4.669999999999945\n",
      "\n",
      "Iteration 4/100:\n",
      "Weights: [16.36287333 16.36287333 16.36287333 16.36287333 16.36287333 16.36287333]\n",
      "Bias: -6.1399999999999135\n",
      "\n",
      "Iteration 5/100:\n",
      "Weights: [19.36163443 19.36163443 19.36163443 19.36163443 19.36163443 19.36163443]\n",
      "Bias: -7.609999999999882\n",
      "\n",
      "Iteration 6/100:\n",
      "Weights: [22.0706602 22.0706602 22.0706602 22.0706602 22.0706602 22.0706602]\n",
      "Bias: -9.07999999999985\n",
      "\n",
      "Iteration 7/100:\n",
      "Weights: [24.58291327 24.58291327 24.58291327 24.58291327 24.58291327 24.58291327]\n",
      "Bias: -10.569999999999819\n",
      "\n",
      "Iteration 8/100:\n",
      "Weights: [26.94474088 26.94474088 26.94474088 26.94474088 26.94474088 26.94474088]\n",
      "Bias: -12.089999999999787\n",
      "\n",
      "Iteration 9/100:\n",
      "Weights: [29.07837262 29.07837262 29.07837262 29.07837262 29.07837262 29.07837262]\n",
      "Bias: -13.609999999999754\n",
      "\n",
      "Iteration 10/100:\n",
      "Weights: [31.0058564 31.0058564 31.0058564 31.0058564 31.0058564 31.0058564]\n",
      "Bias: -15.129999999999722\n",
      "\n",
      "Iteration 11/100:\n",
      "Weights: [32.74710988 32.74710988 32.74710988 32.74710988 32.74710988 32.74710988]\n",
      "Bias: -16.649999999999803\n",
      "\n",
      "Iteration 12/100:\n",
      "Weights: [34.32012634 34.32012634 34.32012634 34.32012634 34.32012634 34.32012634]\n",
      "Bias: -18.17000000000004\n",
      "\n",
      "Iteration 13/100:\n",
      "Weights: [35.74116055 35.74116055 35.74116055 35.74116055 35.74116055 35.74116055]\n",
      "Bias: -19.69000000000028\n",
      "\n",
      "Iteration 14/100:\n",
      "Weights: [37.02489679 37.02489679 37.02489679 37.02489679 37.02489679 37.02489679]\n",
      "Bias: -21.210000000000516\n",
      "\n",
      "Iteration 15/100:\n",
      "Weights: [38.18460056 38.18460056 38.18460056 38.18460056 38.18460056 38.18460056]\n",
      "Bias: -22.730000000000754\n",
      "\n",
      "Iteration 16/100:\n",
      "Weights: [39.42716293 39.42716293 39.42716293 39.42716293 39.42716293 39.42716293]\n",
      "Bias: -24.310000000001\n",
      "\n",
      "Iteration 17/100:\n",
      "Weights: [40.62367976 40.62367976 40.62367976 40.62367976 40.62367976 40.62367976]\n",
      "Bias: -25.96000000000126\n",
      "\n",
      "Iteration 18/100:\n",
      "Weights: [41.72460212 41.72460212 41.72460212 41.72460212 41.72460212 41.72460212]\n",
      "Bias: -27.63000000000152\n",
      "\n",
      "Iteration 19/100:\n",
      "Weights: [42.71915519 42.71915519 42.71915519 42.71915519 42.71915519 42.71915519]\n",
      "Bias: -29.30000000000178\n",
      "\n",
      "Iteration 20/100:\n",
      "Weights: [43.61761618 43.61761618 43.61761618 43.61761618 43.61761618 43.61761618]\n",
      "Bias: -30.97000000000204\n",
      "\n",
      "Iteration 21/100:\n",
      "Weights: [44.52150776 44.52150776 44.52150776 44.52150776 44.52150776 44.52150776]\n",
      "Bias: -32.65000000000207\n",
      "\n",
      "Iteration 22/100:\n",
      "Weights: [45.3639486 45.3639486 45.3639486 45.3639486 45.3639486 45.3639486]\n",
      "Bias: -34.34000000000174\n",
      "\n",
      "Iteration 23/100:\n",
      "Weights: [46.12499421 46.12499421 46.12499421 46.12499421 46.12499421 46.12499421]\n",
      "Bias: -36.0300000000014\n",
      "\n",
      "Iteration 24/100:\n",
      "Weights: [46.81250886 46.81250886 46.81250886 46.81250886 46.81250886 46.81250886]\n",
      "Bias: -37.720000000001065\n",
      "\n",
      "Iteration 25/100:\n",
      "Weights: [47.43359697 47.43359697 47.43359697 47.43359697 47.43359697 47.43359697]\n",
      "Bias: -39.41000000000073\n",
      "\n",
      "Iteration 26/100:\n",
      "Weights: [47.99467658 47.99467658 47.99467658 47.99467658 47.99467658 47.99467658]\n",
      "Bias: -41.10000000000039\n",
      "\n",
      "Iteration 27/100:\n",
      "Weights: [48.50154561 48.50154561 48.50154561 48.50154561 48.50154561 48.50154561]\n",
      "Bias: -42.790000000000056\n",
      "\n",
      "Iteration 28/100:\n",
      "Weights: [48.99272915 48.99272915 48.99272915 48.99272915 48.99272915 48.99272915]\n",
      "Bias: -44.48999999999972\n",
      "\n",
      "Iteration 29/100:\n",
      "Weights: [49.49513748 49.49513748 49.49513748 49.49513748 49.49513748 49.49513748]\n",
      "Bias: -46.209999999999376\n",
      "\n",
      "Iteration 30/100:\n",
      "Weights: [50.15171647 50.15171647 50.15171647 50.15171647 50.15171647 50.15171647]\n",
      "Bias: -48.00999999999902\n",
      "\n",
      "Iteration 31/100:\n",
      "Weights: [50.74485788 50.74485788 50.74485788 50.74485788 50.74485788 50.74485788]\n",
      "Bias: -49.80999999999866\n",
      "\n",
      "Iteration 32/100:\n",
      "Weights: [51.28069095 51.28069095 51.28069095 51.28069095 51.28069095 51.28069095]\n",
      "Bias: -51.6099999999983\n",
      "\n",
      "Iteration 33/100:\n",
      "Weights: [51.76475272 51.76475272 51.76475272 51.76475272 51.76475272 51.76475272]\n",
      "Bias: -53.40999999999794\n",
      "\n",
      "Iteration 34/100:\n",
      "Weights: [52.20204524 52.20204524 52.20204524 52.20204524 52.20204524 52.20204524]\n",
      "Bias: -55.209999999997585\n",
      "\n",
      "Iteration 35/100:\n",
      "Weights: [52.59708729 52.59708729 52.59708729 52.59708729 52.59708729 52.59708729]\n",
      "Bias: -57.00999999999723\n",
      "\n",
      "Iteration 36/100:\n",
      "Weights: [52.95396102 52.95396102 52.95396102 52.95396102 52.95396102 52.95396102]\n",
      "Bias: -58.80999999999687\n",
      "\n",
      "Iteration 37/100:\n",
      "Weights: [53.27635421 53.27635421 53.27635421 53.27635421 53.27635421 53.27635421]\n",
      "Bias: -60.60999999999651\n",
      "\n",
      "Iteration 38/100:\n",
      "Weights: [53.5675983 53.5675983 53.5675983 53.5675983 53.5675983 53.5675983]\n",
      "Bias: -62.40999999999615\n",
      "\n",
      "Iteration 39/100:\n",
      "Weights: [53.83070286 53.83070286 53.83070286 53.83070286 53.83070286 53.83070286]\n",
      "Bias: -64.20999999999594\n",
      "\n",
      "Iteration 40/100:\n",
      "Weights: [54.26624232 54.26624232 54.26624232 54.26624232 54.26624232 54.26624232]\n",
      "Bias: -66.04999999999688\n",
      "\n",
      "Iteration 41/100:\n",
      "Weights: [54.65970068 54.65970068 54.65970068 54.65970068 54.65970068 54.65970068]\n",
      "Bias: -67.88999999999783\n",
      "\n",
      "Iteration 42/100:\n",
      "Weights: [55.06651069 55.06651069 55.06651069 55.06651069 55.06651069 55.06651069]\n",
      "Bias: -69.7799999999988\n",
      "\n",
      "Iteration 43/100:\n",
      "Weights: [55.4340154 55.4340154 55.4340154 55.4340154 55.4340154 55.4340154]\n",
      "Bias: -71.66999999999976\n",
      "\n",
      "Iteration 44/100:\n",
      "Weights: [55.7660124 55.7660124 55.7660124 55.7660124 55.7660124 55.7660124]\n",
      "Bias: -73.56000000000073\n",
      "\n",
      "Iteration 45/100:\n",
      "Weights: [56.06593241 56.06593241 56.06593241 56.06593241 56.06593241 56.06593241]\n",
      "Bias: -75.4500000000017\n",
      "\n",
      "Iteration 46/100:\n",
      "Weights: [56.57569745 56.57569745 56.57569745 56.57569745 56.57569745 56.57569745]\n",
      "Bias: -77.38000000000268\n",
      "\n",
      "Iteration 47/100:\n",
      "Weights: [57.03620983 57.03620983 57.03620983 57.03620983 57.03620983 57.03620983]\n",
      "Bias: -79.31000000000367\n",
      "\n",
      "Iteration 48/100:\n",
      "Weights: [57.45222826 57.45222826 57.45222826 57.45222826 57.45222826 57.45222826]\n",
      "Bias: -81.24000000000466\n",
      "\n",
      "Iteration 49/100:\n",
      "Weights: [57.82805169 57.82805169 57.82805169 57.82805169 57.82805169 57.82805169]\n",
      "Bias: -83.17000000000564\n",
      "\n",
      "Iteration 50/100:\n",
      "Weights: [58.16756367 58.16756367 58.16756367 58.16756367 58.16756367 58.16756367]\n",
      "Bias: -85.10000000000663\n",
      "\n",
      "Iteration 51/100:\n",
      "Weights: [58.47427258 58.47427258 58.47427258 58.47427258 58.47427258 58.47427258]\n",
      "Bias: -87.03000000000762\n",
      "\n",
      "Iteration 52/100:\n",
      "Weights: [58.79078434 58.79078434 58.79078434 58.79078434 58.79078434 58.79078434]\n",
      "Bias: -88.97000000000861\n",
      "\n",
      "Iteration 53/100:\n",
      "Weights: [59.07671526 59.07671526 59.07671526 59.07671526 59.07671526 59.07671526]\n",
      "Bias: -90.9100000000096\n",
      "\n",
      "Iteration 54/100:\n",
      "Weights: [59.52750006 59.52750006 59.52750006 59.52750006 59.52750006 59.52750006]\n",
      "Bias: -92.91000000001063\n",
      "\n",
      "Iteration 55/100:\n",
      "Weights: [59.93473078 59.93473078 59.93473078 59.93473078 59.93473078 59.93473078]\n",
      "Bias: -94.91000000001165\n",
      "\n",
      "Iteration 56/100:\n",
      "Weights: [60.30261554 60.30261554 60.30261554 60.30261554 60.30261554 60.30261554]\n",
      "Bias: -96.91000000001267\n",
      "\n",
      "Iteration 57/100:\n",
      "Weights: [60.63495589 60.63495589 60.63495589 60.63495589 60.63495589 60.63495589]\n",
      "Bias: -98.9100000000137\n",
      "\n",
      "Iteration 58/100:\n",
      "Weights: [60.93518606 60.93518606 60.93518606 60.93518606 60.93518606 60.93518606]\n",
      "Bias: -100.91000000001472\n",
      "\n",
      "Iteration 59/100:\n",
      "Weights: [61.20640849 61.20640849 61.20640849 61.20640849 61.20640849 61.20640849]\n",
      "Bias: -102.91000000001574\n",
      "\n",
      "Iteration 60/100:\n",
      "Weights: [61.45142586 61.45142586 61.45142586 61.45142586 61.45142586 61.45142586]\n",
      "Bias: -104.91000000001677\n",
      "\n",
      "Iteration 61/100:\n",
      "Weights: [61.67277005 61.67277005 61.67277005 61.67277005 61.67277005 61.67277005]\n",
      "Bias: -106.91000000001779\n",
      "\n",
      "Iteration 62/100:\n",
      "Weights: [61.87272834 61.87272834 61.87272834 61.87272834 61.87272834 61.87272834]\n",
      "Bias: -108.91000000001881\n",
      "\n",
      "Iteration 63/100:\n",
      "Weights: [62.05336699 62.05336699 62.05336699 62.05336699 62.05336699 62.05336699]\n",
      "Bias: -110.91000000001983\n",
      "\n",
      "Iteration 64/100:\n",
      "Weights: [62.21655263 62.21655263 62.21655263 62.21655263 62.21655263 62.21655263]\n",
      "Bias: -112.91000000002086\n",
      "\n",
      "Iteration 65/100:\n",
      "Weights: [62.42206698 62.42206698 62.42206698 62.42206698 62.42206698 62.42206698]\n",
      "Bias: -114.92000000002189\n",
      "\n",
      "Iteration 66/100:\n",
      "Weights: [62.73031057 62.73031057 62.73031057 62.73031057 62.73031057 62.73031057]\n",
      "Bias: -117.00000000002295\n",
      "\n",
      "Iteration 67/100:\n",
      "Weights: [63.0253242 63.0253242 63.0253242 63.0253242 63.0253242 63.0253242]\n",
      "Bias: -119.09000000002402\n",
      "\n",
      "Iteration 68/100:\n",
      "Weights: [63.29183409 63.29183409 63.29183409 63.29183409 63.29183409 63.29183409]\n",
      "Bias: -121.18000000002509\n",
      "\n",
      "Iteration 69/100:\n",
      "Weights: [63.65046682 63.65046682 63.65046682 63.65046682 63.65046682 63.65046682]\n",
      "Bias: -123.28000000002616\n",
      "\n",
      "Iteration 70/100:\n",
      "Weights: [63.97444905 63.97444905 63.97444905 63.97444905 63.97444905 63.97444905]\n",
      "Bias: -125.38000000002724\n",
      "\n",
      "Iteration 71/100:\n",
      "Weights: [64.35047697 64.35047697 64.35047697 64.35047697 64.35047697 64.35047697]\n",
      "Bias: -127.49000000002832\n",
      "\n",
      "Iteration 72/100:\n",
      "Weights: [64.69017369 64.69017369 64.69017369 64.69017369 64.69017369 64.69017369]\n",
      "Bias: -129.6000000000271\n",
      "\n",
      "Iteration 73/100:\n",
      "Weights: [64.99704947 64.99704947 64.99704947 64.99704947 64.99704947 64.99704947]\n",
      "Bias: -131.7100000000252\n",
      "\n",
      "Iteration 74/100:\n",
      "Weights: [65.27427542 65.27427542 65.27427542 65.27427542 65.27427542 65.27427542]\n",
      "Bias: -133.82000000002327\n",
      "\n",
      "Iteration 75/100:\n",
      "Weights: [65.52471627 65.52471627 65.52471627 65.52471627 65.52471627 65.52471627]\n",
      "Bias: -135.93000000002135\n",
      "\n",
      "Iteration 76/100:\n",
      "Weights: [65.75095993 65.75095993 65.75095993 65.75095993 65.75095993 65.75095993]\n",
      "Bias: -138.04000000001943\n",
      "\n",
      "Iteration 77/100:\n",
      "Weights: [66.06565853 66.06565853 66.06565853 66.06565853 66.06565853 66.06565853]\n",
      "Bias: -140.1700000000175\n",
      "\n",
      "Iteration 78/100:\n",
      "Weights: [66.4019628 66.4019628 66.4019628 66.4019628 66.4019628 66.4019628]\n",
      "Bias: -142.31000000001555\n",
      "\n",
      "Iteration 79/100:\n",
      "Weights: [66.70577391 66.70577391 66.70577391 66.70577391 66.70577391 66.70577391]\n",
      "Bias: -144.4500000000136\n",
      "\n",
      "Iteration 80/100:\n",
      "Weights: [66.9802313 66.9802313 66.9802313 66.9802313 66.9802313 66.9802313]\n",
      "Bias: -146.59000000001166\n",
      "\n",
      "Iteration 81/100:\n",
      "Weights: [67.22817106 67.22817106 67.22817106 67.22817106 67.22817106 67.22817106]\n",
      "Bias: -148.7300000000097\n",
      "\n",
      "Iteration 82/100:\n",
      "Weights: [67.4521553 67.4521553 67.4521553 67.4521553 67.4521553 67.4521553]\n",
      "Bias: -150.87000000000776\n",
      "\n",
      "Iteration 83/100:\n",
      "Weights: [67.65449854 67.65449854 67.65449854 67.65449854 67.65449854 67.65449854]\n",
      "Bias: -153.01000000000582\n",
      "\n",
      "Iteration 84/100:\n",
      "Weights: [68.02170773 68.02170773 68.02170773 68.02170773 68.02170773 68.02170773]\n",
      "Bias: -155.18000000000384\n",
      "\n",
      "Iteration 85/100:\n",
      "Weights: [68.35343778 68.35343778 68.35343778 68.35343778 68.35343778 68.35343778]\n",
      "Bias: -157.35000000000187\n",
      "\n",
      "Iteration 86/100:\n",
      "Weights: [68.65311661 68.65311661 68.65311661 68.65311661 68.65311661 68.65311661]\n",
      "Bias: -159.5199999999999\n",
      "\n",
      "Iteration 87/100:\n",
      "Weights: [68.92384098 68.92384098 68.92384098 68.92384098 68.92384098 68.92384098]\n",
      "Bias: -161.68999999999792\n",
      "\n",
      "Iteration 88/100:\n",
      "Weights: [69.1684084 69.1684084 69.1684084 69.1684084 69.1684084 69.1684084]\n",
      "Bias: -163.85999999999595\n",
      "\n",
      "Iteration 89/100:\n",
      "Weights: [69.38934613 69.38934613 69.38934613 69.38934613 69.38934613 69.38934613]\n",
      "Bias: -166.02999999999398\n",
      "\n",
      "Iteration 90/100:\n",
      "Weights: [69.58893721 69.58893721 69.58893721 69.58893721 69.58893721 69.58893721]\n",
      "Bias: -168.199999999992\n",
      "\n",
      "Iteration 91/100:\n",
      "Weights: [69.76924414 69.76924414 69.76924414 69.76924414 69.76924414 69.76924414]\n",
      "Bias: -170.36999999999003\n",
      "\n",
      "Iteration 92/100:\n",
      "Weights: [70.17391796 70.17391796 70.17391796 70.17391796 70.17391796 70.17391796]\n",
      "Bias: -172.609999999988\n",
      "\n",
      "Iteration 93/100:\n",
      "Weights: [70.5714992 70.5714992 70.5714992 70.5714992 70.5714992 70.5714992]\n",
      "Bias: -174.85999999998594\n",
      "\n",
      "Iteration 94/100:\n",
      "Weights: [70.9306668 70.9306668 70.9306668 70.9306668 70.9306668 70.9306668]\n",
      "Bias: -177.1099999999839\n",
      "\n",
      "Iteration 95/100:\n",
      "Weights: [71.25513222 71.25513222 71.25513222 71.25513222 71.25513222 71.25513222]\n",
      "Bias: -179.35999999998185\n",
      "\n",
      "Iteration 96/100:\n",
      "Weights: [71.54824832 71.54824832 71.54824832 71.54824832 71.54824832 71.54824832]\n",
      "Bias: -181.6099999999798\n",
      "\n",
      "Iteration 97/100:\n",
      "Weights: [71.81304404 71.81304404 71.81304404 71.81304404 71.81304404 71.81304404]\n",
      "Bias: -183.85999999997776\n",
      "\n",
      "Iteration 98/100:\n",
      "Weights: [72.139917 72.139917 72.139917 72.139917 72.139917 72.139917]\n",
      "Bias: -186.1199999999757\n",
      "\n",
      "Iteration 99/100:\n",
      "Weights: [72.43520803 72.43520803 72.43520803 72.43520803 72.43520803 72.43520803]\n",
      "Bias: -188.37999999997365\n",
      "\n",
      "Iteration 100/100:\n",
      "Weights: [72.70196854 72.70196854 72.70196854 72.70196854 72.70196854 72.70196854]\n",
      "Bias: -190.6399999999716\n",
      "\n",
      "Trained weights (w): [72.70196854 72.70196854 72.70196854 72.70196854 72.70196854 72.70196854]\n",
      "Trained bias (b): -190.6399999999716\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, w, b, learning_rate, n_iterations):\n",
    "    # Melakukan iterasi sebanyak n_iterations\n",
    "    for iteration in range(n_iterations):\n",
    "        # Iterasi melalui setiap sampel data\n",
    "        for idx, x_i in enumerate(X):\n",
    "            # Menghitung kondisi apakah sampel terklasifikasi dengan benar (margin >= 1)\n",
    "            if y[idx] * (np.dot(x_i, w) + b) >= 1:\n",
    "                # Jika terklasifikasi dengan benar, hanya terapkan regularisasi pada bobot\n",
    "                w -= learning_rate * (2 * 1/n_iterations * w)\n",
    "            else:\n",
    "                # Jika terklasifikasi salah, perbarui bobot dan bias untuk mengurangi kesalahan\n",
    "                w -= learning_rate * (2 * 1/n_iterations * w - np.dot(x_i, y[idx] * x_i))\n",
    "                b -= learning_rate * y[idx]\n",
    "        \n",
    "        # Cetak bobot dan bias pada setiap iterasi\n",
    "        print(f\"Iteration {iteration + 1}/{n_iterations}:\")\n",
    "        print(f\"Weights: {w}\")\n",
    "        print(f\"Bias: {b}\\n\")\n",
    "    \n",
    "    # Mengembalikan bobot dan bias yang telah diperbarui\n",
    "    return w, b\n",
    "\n",
    "# Inisialisasi parameter\n",
    "w = np.zeros(X_train_scaled.shape[1])  # Bobot awal diinisialisasi dengan nilai nol untuk semua fitur\n",
    "b = 0  # Bias awal diinisialisasi dengan nilai nol\n",
    "learning_rate = 0.01  # Learning rate menentukan seberapa besar langkah yang diambil pada setiap iterasi\n",
    "n_iterations = 100  # Jumlah iterasi menentukan berapa kali proses pembaruan bobot dan bias dilakukan (gunakan 10 untuk mencetak lebih sedikit iterasi)\n",
    "\n",
    "# Melakukan gradient descent untuk memperbarui bobot dan bias\n",
    "w, b = gradient_descent(X_train_scaled.to_numpy(), y_train.to_numpy(), w, b, learning_rate, n_iterations)\n",
    "\n",
    "# Menampilkan parameter setelah pelatihan\n",
    "print(\"Trained weights (w):\", w)\n",
    "print(\"Trained bias (b):\", b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.047244094488188976\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk melakukan prediksi menggunakan model SVM yang telah dilatih\n",
    "def predict(X, w, b):\n",
    "    return np.sign(np.dot(X, w) + b)\n",
    "# Melakukan prediksi pada data uji\n",
    "y_pred = predict(X_test_scaled.to_numpy(), w, b)\n",
    "\n",
    "# Menghitung akurasi model\n",
    "accuracy = np.mean(y_pred == y_test.to_numpy())\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.021653543307086614\n"
     ]
    }
   ],
   "source": [
    "# Melakukan prediksi pada data latih\n",
    "y_train_pred = predict(X_train_scaled.to_numpy(), w, b)\n",
    "\n",
    "# Menghitung akurasi model pada data latih\n",
    "train_accuracy = np.mean(y_train_pred == y_train.to_numpy())\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to svm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Simpan bobot (weights) dan bias model ke file\n",
    "model_path = 'svm_model.pkl'\n",
    "joblib.dump((w, b), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded:\n",
      "Weights: [72.70196854 72.70196854 72.70196854 72.70196854 72.70196854 72.70196854]\n",
      "Bias: -190.6399999999716\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Muat bobot (weights) dan bias model dari file\n",
    "w_loaded, b_loaded = joblib.load(model_path)\n",
    "\n",
    "print(\"Model loaded:\")\n",
    "print(f\"Weights: {w_loaded}\")\n",
    "print(f\"Bias: {b_loaded}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
